---
title: "ISL Chapter 4: Lab & Applied Exercises"
author: "Momoko Price"
date: "04/05/2020"
output: 
  html_document:
    css: ch4.css
---
***
```{r default_setup, include=FALSE}
knitr::opts_chunk$set(include = T,
                      echo = F, 
                      message = F, 
                      warning = F,
                      fig.align = 'left',
                      out.width = '80%')

# global options
options(stringsAsFactors = F)
# Load libraries
require(tidyverse)
require(data.table)
require(ggplot2)
require(gridExtra)
require(PerformanceAnalytics)
require(magrittr)
require(MASS)

# load custom functions
get.cors <- function(arg1, arg2) {
  # check for correlations (remove 'Purchase' var bc it's qualitative)
  cors <- as.data.frame(cor(arg1[,-arg2]))
  # Sort the correlations and see which variables have the strongest correlations: 
  # turn rownames into a variable
  setDT(cors, keep.rownames = "var1")
  # gather corrs into single column
  cors_tidy <- cors %>% 
    pivot_longer(c(2:arg2), names_to = "var2", values_to = "cors") %>% 
    # remove self-correlations
    filter(cors != 1) %>%
    # sort from highest to lowest
    arrange(desc(cors))
  # remove even rows (duplicates)
  toDelete <- seq(1, nrow(cors_tidy), 2)
  cors <- cors_tidy[-toDelete, ]
  return(cors)
}
```

```{r extras, echo=F}
# load custom libraries
require(ISLR)
require(MASS) # for lda, qda modeling
require(class) # for KNN modeling
```

```{r data, echo=F}
# load data
data(Smarket)
```
#### 4.6 Lab: Logistic Regression, LDA, QDA, and KNN

##### 4.6.1 The Stock Market Data
We will begin by examining some numerical and graphical summaries of the Smarket data, which is part of the ISLR library. This data set consists of percentage returns for the S&P 500 stock index over 1, 250 days, from the beginning of 2001 until the end of 2005. For each date, we have recorded the percentage returns for each of the five previous trading days, Lag1 through Lag5. We have also recorded Volume (the number of shares traded on the previous day, in billions), Today (the percentage return on the date in question) and Direction (whether the market was Up or Down on this date).
```{r check_data, echo=T}
# check structure
str(Smarket)
# check max/min & quantiles of each variable
summary(Smarket)
# facet the variables against one another & look for trends
pairs(Smarket)
# check for correlations (remove 'Direction' var bc it's qualitative)
cor(Smarket[,-9])

```

``` {r save_plots1, include=F}
# save larger version of plots to images folder
png(filename="images/check_data_pairs.png", width = 1900, height = 1024)
pairs(Smarket)
dev.off()
```
Sort the correlations and see which variables have the strongest correlations: 
``` {r sort_gather, echo=T}
# convert to dataframe
corr <- as.data.frame(cor(Smarket[,-9]))
# turn rownames into a variable
setDT(corr, keep.rownames = "var1")
# gather corrs into single column
corr_tidy <- corr %>% 
  pivot_longer(c(2:9), names_to = "var2", values_to = "corr") %>% 
  # remove self-correlations
  filter(corr != 1) %>%
  # sort from highest to lowest
  arrange(desc(corr))
# remove even rows (duplicates)
toDelete <- seq(1, nrow(corr_tidy), 2)
corrs <- corr_tidy[-toDelete, ]
head(corrs, 9)
```
There isn't much of an association between any variable except year and volume. Let's see what that looks like:
``` {r plot_volume, echo=T}
plot(Smarket$Year, Smarket$Volume, xlab = "Year", ylab = "Volume")
```

##### 4.6.2 Logistic Regression
Next, we will fit a logistic regression model in order to predict Direction using Lag1 through Lag5 and Volume. The glm() function fits generalized linear models, a class of models that includes logistic regression. The syntax of the glm() function is similar to that of lm(), except that we must pass in the argument family=binomial in order to tell R to run a logistic regression rather than some other type of generalized linear model.

``` {r glm_logistic_fit, echo=T}
# predict direction based on lag variables and trade volume
stock.1 <- glm(Direction ~ . -Year -Today, data = Smarket, family = binomial)
summary(stock.1)
```

``` {r access_model_vals, include=F}
# try different ways to access parts of the model
# get all the coefficients
coef(stock.1)

# get just the volume value, no key
coef(stock.1)[[7]]
```

According to this model, there is no significant relationship between the outcome (direction of the stock on a given date) and the % change of any of the previous 5 days. 

```{r predict_up, echo=T}
stock.1.probs <- predict(stock.1, type="response")
stock.1.probs[1:10]
```

```{r label_by_qual, echo=T}
# change prob values to clear 'up' or 'down' values
# first make a vector of 'down' values the same length as the Smarket sample
len <- length(Smarket)
stock.1.preds <- rep("Down", 1250)
stock.1.preds[stock.1.probs > .5] ="Up"
stock.1.preds[1:10]
```

How do the predictions from our model compare with the actual direction values for the Smarket dataset? We can use the 'table' function to create a confusion matrix that compares the number of days whose directions were correctly predicted vs those that weren't. 

```{r confusion_matrix, echo=T}
# create confusion matrix
table(stock.1.preds, Smarket$Direction)
# calculate the % of correct predictions using the mean() function
mean(stock.1.preds == Smarket$Direction)
# calculate the % of incorrect predictions using the mean() function
mean(stock.1.preds != Smarket$Direction)
```

This looks like it might be a little better than random, but remember it's the training error rate, not the testing error rate! To get a better estimate of model predictability, what you'd need to do is set aside a sample of data that won't be included in building the model and then use that to generate model outcomes based on the sample's predictors and check how accurate they are, like so: 

```{r create_testing_data, echo=T}
# create a vector of logical values that label each obs as either before (T) or during (F) 2005
train <- (Smarket$Year < 2005)
test <- (Smarket$Year == 2005)
# use this to create a subset of Smarket data that's only those IN 2005
Smarket.2005 <- Smarket[test,]
# create a single-var vector of test predictions from this data
Direction.2005 <- Smarket.2005$Direction
```

Now we set a new model using ONLY data from before 2005 (i.e. our training data), like so: 

```{r train_test_log_model, echo=T}
# create the model (i.e. "train")
pre2005.fit <- glm(Direction ~ . -Today -Year, data=Smarket, family=binomial, subset=train)
# generate predictions from the model(i.e. "test")
pre2005.fit.probs <- predict(pre2005.fit, Smarket.2005, type="response")
# convert to qualitative labels ("Up", "Down")
pre2005.fit.preds <- rep("Down", 252)
pre2005.fit.preds[pre2005.fit.probs >0.5] <- "Up"
table(pre2005.fit.preds, Direction.2005)
mean(pre2005.fit.preds == Direction.2005)
```
This model has an accuracy of less than 50% (or a test error rate of 52%) - not great! How can we improve this model? One way would be to remove the predictors with the highest p values. Let's see what happens if we do this. 
```{r remove_high_pvals, echo=T}
# identify the higher p-values
summary(pre2005.fit)
# train new model only using Lag1 and Lag2 as predictors
lag1lag2.fit <- glm(Direction ~ Lag1 + Lag2, data = Smarket, family = binomial, subset = train)
# test new model with test data
lag1lag2.fit.probs <- predict(lag1lag2.fit, Smarket.2005, type="response")
# convert to qual values
lag1lag2.fit.preds <- rep("Down", length(Smarket.2005))
lag1lag2.fit.preds[lag1lag2.fit.probs > 0.5] <- "Up"
lag1lag2.fit.preds[lag1lag2.fit.probs <= 0.5] <- "Down"
# create confusion matrix to gauge error rate (predictions vs. test data)
table(lag1lag2.fit.preds, Direction.2005)
mean(lag1lag2.fit.preds != Direction.2005)
```
Test error rate is slightly better, but not great. But better than before! Using this model, let's use the predict function to actually make a prediction of what the stock direction would be given an exact-value vector of predictors. 

```{r predict_exact, echo=T}
# test new model with an exact-value vector
exact.probs <- predict(lag1lag2.fit, newdata= data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)), type="response")
# convert to qual values
exact.preds <- rep("Down", length(exact.probs))
exact.preds[exact.probs > 0.5] <- "Up"
```

##### 4.6.3 Linear Discriminant Analysis

Let's now try building a predictive model on the Smarket data (just using Lag1 and Lag2 as predictors), using Linear Discriminant Analysis:

```{r train_lda, echo=t}
# train model with pre2005 data, using the subset function and the 'train' vector
lda.fit <- lda(Direction ~ Lag1 + Lag2, data=Smarket, subset=train)
# note that for LDA you don't have to use 'summary' to get the analysis
lda.fit
# plot the linear discriminants, which means the histogram distributions of y values with the linear discriminant function of y = -0.642*Lag1-0.514*Lag2.  
plot(lda.fit)
```
The LDA plot gives us a general perspective of how distinct th distribution of training values are between the two classes, using the linear model we've built using Lag1 and Lag2 as our predictors. (Clearly there is not much of a difference between the two, but the distribution of down values is lower overall than that of the up values.)

Let's now test the model using our test data:
```{r test_lda, echo=T}
lda.pred.values <- predict(lda.fit, Smarket.2005)
lda.preds <- lda.pred.values$class
table(lda.preds, Direction.2005)
mean(lda.preds == Direction.2005)
```
Note that 'posterior probability' refers to the probability of an outcome/prediction GIVEN a specific set of predictors.

##### 4.6.4 Quadratic Discriminant Analysis
QDA is basically the same as LDA in concept, it just has less bias, which allows for better fitting, but leaves it vulnerable to overfitting as well. Let's fit the Smarket data to a QDA model and see how accurately it predicts: 
```{r train_qda, echo=T}
qda.fit <- qda(Direction ~ Lag1 + Lag2, data=Smarket, subset=train)
qda.fit
```
```{r test_qda, echo=TRUE}
qda.preds <- predict(qda.fit, Smarket.2005)$class
table(qda.preds, Direction.2005)
mean(qda.preds == Direction.2005)
```
##### 4.6.5 K-Nearest Neighbours
We will now perform KNN using the knn() function, which is part of the class library. This function works rather differently from the other model- fitting functions that we have encountered thus far. Rather than a two-step approach in which we first fit the model and then we use the model to make predictions, knn() forms predictions using a single command, and requires four inputs.

```{r knn, echo=T}
# set up your data - first the matrix of predictors (which will get split into training and testing subsets when input into the knn model)
Smarket.knn <- cbind(Smarket$Lag1, Smarket$Lag2)
# next, your vector of training outcomes
Direction.pre2005 <- Smarket$Direction[train]

set.seed(1)
# order of inputs: training predictors, test predictors, training outcomes, # of nearest neighbours
# output of knn is just a vector of predicted outcomes
knn.pred <- knn(Smarket.knn[train,], 
                Smarket.knn[!train,], 
                Direction.pre2005, 
                k=1)
# test the accuracy against the test outcomes
table(knn.pred, Direction.2005)
mean(knn.pred == Direction.2005)
```
Hm, the accuracy of the predictions are clearly pretty garbage (same as random). Let's try increasing the # of nearest neighbours to 3 and see what happens
```{r knn_k3, echo=T}
knn.pred.k3 <- knn(Smarket.knn[train,], 
                Smarket.knn[!train,], 
                Direction.pre2005, 
                k=3)
# test the accuracy against the test outcomes
table(knn.pred.k3, Direction.2005)
mean(knn.pred.k3 == Direction.2005)
```
Hm, by increasing K we can get an accuracy of about 54% - so out of the 4 tests we ran (logistical regression, LDA, QDA, and KNN), QDA is our best model (accuracy = 56%) using 2 predictors. 

##### 4.6.6 An Application to Caravan Insurance Data
Finally, we will apply the KNN approach to the Caravan data set, which is part of the ISLR library. This data set includes 85 predictors that measure demographic characteristics for 5,822 individuals. The response variable is Purchase, which indicates whether or not a given individual purchases a caravan insurance policy. In this data set, only 6% of people purchased caravan insurance.

```{r caravan_data, echo=F}
# load data
data(Caravan)
```
```{r explore_caravan, echo=T}
# check structure
str(Caravan)
# check max/min & quantiles of each variable
summary(Caravan)
# facet the variables against one another & look for trends
## pairs(Caravan) - don't bother, too many plots to fit space
# check for correlations (remove 'Purchase' var bc it's qualitative)
cors <- as.data.frame(cor(Caravan[,-86]))
# Sort the correlations and see which variables have the strongest correlations: 
# turn rownames into a variable
setDT(cors, keep.rownames = "var1")
# gather corrs into single column
cors_tidy <- cors %>% 
  pivot_longer(c(2:86), names_to = "var2", values_to = "cors") %>% 
  # remove self-correlations
 filter(cors != 1) %>%
# sort from highest to lowest
  arrange(desc(cors))
# remove even rows (duplicates)
toDelete <- seq(1, nrow(cors_tidy), 2)
cors <- cors_tidy[-toDelete, ]
head(cors, 9)
tail(cors, 9)
# how many people actually purchased a caravan? 
purchased <- summary(Caravan$Purchase)[['Yes']]
# get the conversion rate
purchased/nrow(Caravan)
```
Well that initial exploration shows us that some predictors are HIGHLY correlated, both positively and negatively. Won't do anything about that now. Let's prep the data for KNN, given that the variable 'Purchase' is our qualitative outcome. 

The next step in our KNN analysis is to *standardize* our predictors using the 'scale' function. 
Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Any variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale. For instance, imagine a data set that contains two variables, salary and age (measured in dollars and years, respectively). As far as KNN is concerned, a difference of \$1,000 in salary is enormous compared to a difference of 50 years in age. Consequently, salary will drive the KNN classification results, and age will have almost no effect. This is contrary to our intuition that a salary difference of \$1,000 is quite small compared to an age difference of 50 years. Furthermore, the importance of scale to the KNN classifier leads to another issue: if we measured salary in Japanese yen, or if we measured age in minutes, then we’d get quite different classification results from what we get if these two variables are measured in dollars and years.
A good way to handle this problem is to standardize the data so that all variables are given a mean of zero and a standard deviation of one. Then all variables will be on a comparable scale. The scale() function does just this. In standardizing the data, we exclude column 86, because that is the qualitative Purchase variable.
```{r standardize, echo=T}
# scale the 85 variables using the scale() function
scaled.Caravan <- as.data.frame(scale(Caravan[,1:85]))
scaled.Caravan <- cbind(scaled.Caravan, Purchase = Caravan$Purchase)
```
Great, the variables are now all standardized. Next step in the process is to split the data into a test set and training set. We'll use the first 1000 observations as the test set and the remainder as the training set:
```{r create_sets, echo=T}
test <- 1:1000
# predictors
Caravan.X.test <- scaled.Caravan[test,-86]
Caravan.X.train <- scaled.Caravan[-test,-86]
# outcomes
Caravan.Y.test <- scaled.Caravan[test, 86]
Caravan.Y.train <- scaled.Caravan[-test, 86]
```
Done - next step is to run the KNN model, so let's do that now: 
```{r caravan_knn, echo=T}
set.seed(1)
Caravan.knn.pred <- knn(Caravan.X.train, Caravan.X.test, Caravan.Y.train, k = 1)
# confusion matrix
conf.matrix <- table(Caravan.knn.pred, Caravan.Y.test)
# total test error rate
mean(Caravan.knn.pred != Caravan.Y.test)
# purchase rate from the test data
mean(Caravan.Y.test == "Yes")
```
Hm, the total test error rate for our model is around 12%. Is that any good? Not really, because the actual purchase rate of our test sample is only about 5.9%, so we could technically get the same result by just saying 'no' by default (essentially random guessing).

What happens if we increase K?

```{r knn_k17, echo=T}
Caravan.knn.k17 <- knn(Caravan.X.train, Caravan.X.test, Caravan.Y.train, k = 17)
conf.matrix.k17 <- table(Caravan.knn.k17, Caravan.Y.test)
# total test error rate
mean(Caravan.knn.k17 != Caravan.Y.test)
```
***
#### 4.7 Applied Exercises
10. This question should be answered using the Weekly data set, which is part of the ISLR package. This data is similar in nature to the Smarket data from this chapter’s lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.
```{r weekly_data, echo=F}
# load Weekly data
data(Weekly)
```
(a) Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?
```{r explore_weekly, echo=T}
# check structure
str(Weekly)
# check max/min & quantiles of each variable
summary(Weekly)
# get top 5 pos. & neg. correlations among the predictors
head(get.cors(Weekly, 9), 5)
tail(get.cors(Weekly, 9), 5)
# generate boxplots to see if there appears to be a difference in predictor mean between 'up' and 'down'

# https://www.r-graph-gallery.com/265-grouped-boxplot-with-ggplot2.html#:~:text=A%20grouped%20boxplot%20is%20a,called%20in%20the%20fill%20argument.

# pivot data to long form
weekly.long <- Weekly %>%
  pivot_longer(-Direction, names_to = "Predictor", values_to = "Value")
weekly.long <- weekly.long[, c("Predictor", "Direction", "Value")]

# create facet of plots
boxplot <- ggplot(weekly.long, aes(x=Predictor, y=Value, fill=Direction)) +
  geom_boxplot() +
  facet_wrap(~Predictor, scales = "free")
boxplot
```
(b) Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?
```{r glm_today, echo=T}
weekly.log.fit.today <- glm(Direction ~ -Year -Volume, data = Weekly, family = binomial)
summary(weekly.log.fit.today)
```
Note that we did first did a glm fit that includes the 'Today' variable, just to see what's going on with the obvious difference shown in this variable in the box plots. But this model threw an error, ("fitted probabilities numerically 0 or 1 occurred") which indicates that there is a 100% correlation between the predictor and the outcome class. In hindsight, this makes sense, because the Direction class is just a classification that's *based* on the Today predictor.

Other than the wonky Today predictor, there appears to be a significant positive relationship between the Lag2 (the market change two days before), and the outcome:

```{r weekly_glm, echo=T}
weekly.log.fit <- glm(Direction ~ . -Year -Today -Volume, data = Weekly, family = binomial)
summary(weekly.log.fit)
```
(c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.
``` {r weekly_test_train_fit, echo=T}
# set training and test indexes
# create a vector of logical values that label each obs as either before (T) or during and after (F) 2005
train <- (Weekly$Year < 2005)
test <- (Weekly$Year >= 2005)
# use this to create a subset of training data (pre-2005) and test data (2005 and after)
Weekly.test <- Weekly[test,]
Weekly.train <- Weekly[train,]
# create a single-var vector of test predictions from this data
Weekly.test.outcomes <- Weekly.test$Direction
# create a logistical model with the training data
Weekly.glm.train.fit <- glm(Direction ~ . -Year -Today -Volume, data = Weekly.train, family = binomial)
summary(Weekly.glm.train.fit)
```
Interesting - if we split the data into test and training, the relationship with Lag2 loses significance. Let's run some predictions now with our test data set: 
``` {r weekly_glm_preds, echo=F}
# generate new probs
weekly.test.preds <- predict(Weekly.glm.train.fit, newdata = Weekly.test, type="response")
# NEXT: create factor based vector of predictions, then do confusion matrix ...
```