---
title: "ISL Chapter 4.6 Lab: <br>Logistic Regression, LDA, QDA, and KNN"
author: "Momoko Price"
date: "04/05/2020"
output: 
  html_document:
    css: ch4.css
---

```{r default_setup, include=FALSE}
knitr::opts_chunk$set(include = T,
                      echo = F, 
                      message = F, 
                      warning = F,
                      fig.align = 'left',
                      out.width = '80%')

# global options
options(stringsAsFactors = F)
# Load libraries
require(tidyverse)
require(data.table)
require(ggplot2)
require(gridExtra)
require(PerformanceAnalytics)
require(magrittr)
```

```{r extras, echo=F}
# load custom libraries
require(ISLR)
require(MASS)
```

```{r data, echo=F}
# load data
data(Smarket)
```

##### 4.6.1 The Stock Market Data
We will begin by examining some numerical and graphical summaries of the Smarket data, which is part of the ISLR library. This data set consists of percentage returns for the S&P 500 stock index over 1, 250 days, from the beginning of 2001 until the end of 2005. For each date, we have recorded the percentage returns for each of the five previous trading days, Lag1 through Lag5. We have also recorded Volume (the number of shares traded on the previous day, in billions), Today (the percentage return on the date in question) and Direction (whether the market was Up or Down on this date).
```{r check_data, echo=T}
# check structure
str(Smarket)
# check max/min & quantiles of each variable
summary(Smarket)
# facet the variables against one another & look for trends
pairs(Smarket)
# check for correlations (remove 'Direction' var bc it's qualitative)
cor(Smarket[,-9])

```

``` {r save_plots1, include=F}
# save larger version of plots to images folder
png(filename="images/check_data_pairs.png", width = 1900, height = 1024)
pairs(Smarket)
dev.off()
```
Sort the correlations and see which variables have the strongest correlations: 
``` {r sort_gather, echo=T}
# convert to dataframe
corr <- as.data.frame(cor(Smarket[,-9]))
# turn rownames into a variable
setDT(corr, keep.rownames = "var1")
# gather corrs into single column
corr_tidy <- corr %>% 
  pivot_longer(c(2:9), names_to = "var2", values_to = "corr") %>% 
  # remove self-correlations
  filter(corr != 1) %>%
  # sort from highest to lowest
  arrange(desc(corr))
# remove even rows (duplicates)
toDelete <- seq(1, nrow(corr_tidy), 2)
corrs <- corr_tidy[-toDelete, ]
head(corrs, 9)
```
There isn't much of an association between any variable except year and volume. Let's see what that looks like:
``` {r plot_volume, echo=T}
plot(Smarket$Year, Smarket$Volume, xlab = "Year", ylab = "Volume")
```

##### 4.6.2 Logistic Regression
Next, we will fit a logistic regression model in order to predict Direction using Lag1 through Lag5 and Volume. The glm() function fits generalized linear models, a class of models that includes logistic regression. The syntax of the glm() function is similar to that of lm(), except that we must pass in the argument family=binomial in order to tell R to run a logistic regression rather than some other type of generalized linear model.

``` {r glm_logistic_fit, echo=T}
# predict direction based on lag variables and trade volume
stock.1 <- glm(Direction ~ . -Year -Today, data = Smarket, family = binomial)
summary(stock.1)
```

``` {r access_model_vals, include=F}
# try different ways to access parts of the model
# get all the coefficients
coef(stock.1)

# get just the volume value, no key
coef(stock.1)[[7]]
```

According to this model, there is no significant relationship between the outcome (direction of the stock on a given date) and the % change of any of the previous 5 days. 

```{r predict_up, echo=T}
stock.1.probs <- predict(stock.1, type="response")
stock.1.probs[1:10]
```

```{r label_by_qual, echo=T}
# change prob values to clear 'up' or 'down' values
# first make a vector of 'down' values the same length as the Smarket sample
len <- length(Smarket)
stock.1.preds <- rep("Down", 1250)
stock.1.preds[stock.1.probs > .5] ="Up"
stock.1.preds[1:10]
```

How do the predictions from our model compare with the actual direction values for the Smarket dataset? We can use the 'table' function to create a confusion matrix that compares the number of days whose directions were correctly predicted vs those that weren't. 

```{r confusion_matrix, echo=T}
# create confusion matrix
table(stock.1.preds, Smarket$Direction)
# calculate the % of correct predictions using the mean() function
mean(stock.1.preds == Smarket$Direction)
# calculate the % of incorrect predictions using the mean() function
mean(stock.1.preds != Smarket$Direction)
```

This looks like it might be a little better than random, but remember it's the training error rate, not the testing error rate! To get a better estimate of model predictability, what you'd need to do is set aside a sample of data that won't be included in building the model and then use that to generate model outcomes based on the sample's predictors and check how accurate they are, like so: 

```{r create_testing_data, echo=T}
# create a vector of logical values that label each obs as either before (T) or during (F) 2005
train <- (Smarket$Year < 2005)
test <- (Smarket$Year == 2005)
# use this to create a subset of Smarket data that's only those IN 2005
Smarket.2005 <- Smarket[test,]
# create a single-var vector of test predictions from this data
Direction.2005 <- Smarket.2005$Direction
```

Now we set a new model using ONLY data from before 2005 (i.e. our training data), like so: 

```{r train_test_log_model, echo=T}
# create the model (i.e. "train")
pre2005.fit <- glm(Direction ~ . -Today -Year, data=Smarket, family=binomial, subset=train)
# generate predictions from the model(i.e. "test")
pre2005.fit.probs <- predict(pre2005.fit, Smarket.2005, type="response")
# convert to qualitative labels ("Up", "Down")
pre2005.fit.preds <- rep("Down", 252)
pre2005.fit.preds[pre2005.fit.probs >0.5] <- "Up"
table(pre2005.fit.preds, Direction.2005)
mean(pre2005.fit.preds == Direction.2005)
```
This model has an accuracy of less than 50% (or a test error rate of 52%) - not great! How can we improve this model? One way would be to remove the predictors with the highest p values. Let's see what happens if we do this. 
```{r remove_high_pvals, echo=T}
# identify the higher p-values
summary(pre2005.fit)
# train new model only using Lag1 and Lag2 as predictors
lag1lag2.fit <- glm(Direction ~ Lag1 + Lag2, data = Smarket, family = binomial, subset = train)
# test new model with test data
lag1lag2.fit.probs <- predict(lag1lag2.fit, Smarket.2005, type="response")
# convert to qual values
lag1lag2.fit.preds <- rep("Down", length(Smarket.2005))
lag1lag2.fit.preds[lag1lag2.fit.probs > 0.5] <- "Up"
lag1lag2.fit.preds[lag1lag2.fit.probs <= 0.5] <- "Down"
# create confusion matrix to gauge error rate (predictions vs. test data)
table(lag1lag2.fit.preds, Direction.2005)
mean(lag1lag2.fit.preds != Direction.2005)
```
Test error rate is slightly better, but not great. But better than before!